# -*- coding: utf-8 -*-
"""Training_model_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158grAOcl4_UkY9zMT2DN8ZwsMrvAcrr4

# Importing DF
"""

import pandas as pd
import numpy as np

df = pd.read_csv('Chennai houseing sale.csv')
df.head()

df.info()

"""# Removing unessory Columns"""

unwanted_columns = [
    'PRT_ID',
    'DATE_SALE',
    'DATE_BUILD',
    'SALE_COND',
    'BUILDTYPE',
    'UTILITY_AVAIL',
    'STREET',
    'MZZONE',
    'QS_ROOMS',
    'QS_BATHROOM',
    'QS_BEDROOM',
    'QS_OVERALL',
]

df.drop(columns=unwanted_columns  ,inplace = True)

df.info()

"""# Label Encoding

## FILL NAN
"""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')  # Use 'median' for median imputation
df['N_BEDROOM'] = imputer.fit_transform(df[['N_BEDROOM']])
df['N_BATHROOM'] = imputer.fit_transform(df[['N_BATHROOM']])
df['N_ROOM'] = imputer.fit_transform(df[['N_ROOM']])

"""## AREA"""

df

df.AREA.unique()

from collections import Counter
count = Counter(df.AREA)
lis = list(count.keys())
lis.sort()

temp = {
    'Adyar': 'Adyar',
    'Adyr': 'Adyar',
    'Ana Nagar': 'AnnaNagar',
    'Ann Nagar': 'AnnaNagar',
    'Anna Nagar': 'AnnaNagar',
    'Chrompt': 'Chrompet',
    'Chormpet': 'Chrompet',
    'Chrmpet': 'Chrompet',
    'Chrompet': 'Chrompet',
    'KK Nagar': 'KKNagar',
    'KKNagar': 'KKNagar',
    'Karapakam': 'Karapakam',
    'Karapakkam': 'Karapakam',
    'TNagar': 'TNagar',
    'T Nagar': 'TNagar',
    'Velachery': 'Velachery',
    'Velchery': 'Velachery'
}

for i in lis:
  if not  i in temp.keys():
    print(i)

places = set(temp.values())
places

df['AREA'] = df['AREA'].replace(temp)

df.AREA.unique()

d = {}
count = 0
for i , n in enumerate(df.AREA.unique()):
  d[n] = i

df['AREA'] = df['AREA'].replace(d)
d

"""## Parking Facility"""

d = {'Yes' : 1  , 'No' : 0 , 'Noo' : 0}
df['PARK_FACIL'] = df['PARK_FACIL'].replace(d)

df.info()

"""## SALES_PRICE"""

df.head()

df['N_ROOM'] = df['N_ROOM'] - (df['N_BEDROOM'] + df['N_BATHROOM'])
df

df['SALES_PRICE'] = df['SALES_PRICE'] - (df['COMMIS'] + df['REG_FEE'])

min(df.SALES_PRICE),max(df.SALES_PRICE)

df.drop(columns= ['REG_FEE' ,	'COMMIS'], inplace=True)
df

"""## DIST_MAINROAD"""

min(df.DIST_MAINROAD) , max(df.DIST_MAINROAD)

d = {'near' : 1 , 'mid' : 2 , 'far' : 3}
new_values = []
for i in df.DIST_MAINROAD:
  if i < 66:
    new_values.append(1)
  elif i < 132:
    new_values.append(2)
  else:
    new_values.append(3)

df['DIST_MAINROAD'] = new_values
df



"""# Model Training"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.neural_network import MLPRegressor

X = df.iloc[:, :-1]  # X contains all columns except the last one
Y = df.iloc[:, -1]   # Y contains the last column (target variable)

df.info()

df.head()

# Split the dataset into a training set and a testing set
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Create a list of models
models = [
    LinearRegression(),
    Ridge(),
    Lasso(),
    DecisionTreeRegressor(),
    RandomForestRegressor(),
    GradientBoostingRegressor(),
    SVR(),
    KNeighborsRegressor(),
    MLPRegressor(),
]

# Create a list of model names for reference
model_names = [
    'Linear Regression',
    'Ridge',
    'Lasso',
    'Decision Tree',
    'Random Forest',
    'Gradient Boosting',
    'SVR',
    'K-Nearest Neighbors',
    'Neural Network',
]

# Initialize lists to store model accuracies
mse_scores = []
r2_scores = []

# Iterate through each model and evaluate its performance
for model, model_name in zip(models, model_names):
    model.fit(X_train, Y_train)
    Y_pred = model.predict(X_test)

    mse = mean_squared_error(Y_test, Y_pred)
    r2 = r2_score(Y_test, Y_pred)

    mse_scores.append(mse)
    r2_scores.append(r2)

    print(f'{model_name} - Mean Squared Error: {mse}, R-squared (R2) Score: {r2}')

# Find the model with the best R-squared score
best_model_index = np.argmax(r2_scores)
best_model = models[best_model_index]
best_model_name = model_names[best_model_index]
best_r2_score = r2_scores[best_model_index]

print(f'\nBest Model: {best_model_name} with R-squared (R2) Score: {best_r2_score}')

"""# Gradient Boosting model"""

model = GradientBoostingRegressor()
model.fit(X,Y)

Y_pred = model.predict(X)
mse = mean_squared_error(Y, Y_pred)
r2 = r2_score(Y, Y_pred)
r2,mse

model

"""# Download the model"""
try:
    from google.colab import files
except ModuleNotFoundError:
    files = None  # or any other fallback mechanism you want to implement

import pickle

with open('model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)
print("Model saved successfully.")

files.download('model.pkl')